{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "# Data Pre-processing\n",
    "from gtda.time_series import SlidingWindow\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, MaxAbsScaler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# Persistent Homology\n",
    "from gtda.homology import VietorisRipsPersistence\n",
    "from gtda.diagrams import BettiCurve\n",
    "\n",
    "# Artificial neural network\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBettiSeq (VRdiagram_array, dimension:int, binscount = 1000):\n",
    "    '''create and obtain (x,y) coordinates on filtration parameters and betti numbers from the collection of Betti Curve plots representing the PSE Vietoris Rips diagrams'''\n",
    "    # Generating the Betti Curve plot for the Vietoris Rips diagrams\n",
    "    BC = BettiCurve(n_bins=binscount)\n",
    "    bettiNumbers = BC.fit_transform(VRdiagram_array)\n",
    "    if dimension == 0:\n",
    "        for index in range(len(bettiNumbers)):\n",
    "            bettiNumbers[index][0] = [x+1 for x in bettiNumbers[index][0]]\n",
    "    filtrationParameters = list(BC.samplings_.values())\n",
    "\n",
    "    # Creating the tuple for the Betti numbers and Filtration parameters in the Betti Curve plot\n",
    "    bettiCollection = []\n",
    "    for index in range(len(bettiNumbers)):\n",
    "        bettiCoordinates = [\n",
    "            (x, y) \n",
    "            for x, y in zip(filtrationParameters[0], bettiNumbers[index][0])\n",
    "        ]\n",
    "        bettiCollection.append(bettiCoordinates)\n",
    "    return bettiCollection\n",
    "\n",
    "def createBettiSeqPartition (homology_list, size:int):\n",
    "    '''outputs the Betti Sequence from a collection of giotto Betti Curve plot coordinates as a list'''  \n",
    "    #obtaining the minimum and maximum filtration parameters (provided, the parameter range is the whole plot)\n",
    "    minParameter = homology_list[0][0][0]\n",
    "    maxParameter = homology_list[0][-1][0]\n",
    "\n",
    "    #obtaining the middle filtration parameter values of each partition size\n",
    "    midPartition = (maxParameter - minParameter) / size\n",
    "    midPartitionParameters = [\n",
    "        minParameter + ((((2*i) - 1) * midPartition) / 2)\n",
    "        for i in range(1, size + 1)\n",
    "    ]\n",
    "\n",
    "    # creating the Betti sequence of each partition of specified size\n",
    "    sequences = []\n",
    "    for homList in homology_list:\n",
    "        midBettiNumbers = []\n",
    "        for partition in range(size):\n",
    "            midParameter = midPartitionParameters[partition]\n",
    "            for index, element in enumerate(homList):\n",
    "                if element[0] > midParameter: \n",
    "                    midBettiNumbers.append(homList[index-1][1])\n",
    "                    break\n",
    "        sequences.append(midBettiNumbers)\n",
    "\n",
    "    return sequences\n",
    "\n",
    "def filterDataOnYear(data, date_column, year: int):\n",
    "    '''obtaining the data and column based on indicated year'''\n",
    "    yearDates = list(data[date_column][data[date_column].dt.year == year])\n",
    "    filteredData = data.loc[(data[date_column] >= yearDates[0]) & (data[date_column] <= yearDates[-1])]\n",
    "    return filteredData\n",
    "\n",
    "def filterDataSlidingWindow(data, date_column,year:int, window_size: int):\n",
    "    '''obtaining the data to be used for the Sliding Windows'''\n",
    "    yearDates = list(data[date_column][data[date_column].dt.year == year])\n",
    "    prevYearDates = list(data[date_column][data[date_column].dt.year == (year-1)])\n",
    "    prevYearDatesForSW = prevYearDates[(-window_size+1):]\n",
    "    SWDates = prevYearDatesForSW + yearDates\n",
    "    filteredData = data.loc[(data[date_column] >= SWDates[0]) & (data[date_column] <= SWDates[-1])]\n",
    "    return filteredData\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VARIABLES AND PARAMETERS\n",
    "\n",
    "# Years covered\n",
    "startYear = 2012\n",
    "endYearExclusive = 2024 # Range will cover up to this year minus one\n",
    "\n",
    "# Scaler used\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Persistent homology dimensions\n",
    "dimensionPH = [0,1]\n",
    "\n",
    "# Persistent homology sequence parameters (i.e., sliding window size and sequence partition size)\n",
    "windowSizes = [80,100,120]\n",
    "partitionSizes = [30,60,100]\n",
    "\n",
    "# Activation function\n",
    "activationANN = 'sigmoid'\n",
    "# 'ReLU' for Rectified Linear Unit function\n",
    "\n",
    "# Parameter values for Artificial Neural Network model combinations\n",
    "unitParameters = [x for x in range(5,21,5)]\n",
    "epochParameters = [y for y in range(20,101,20)]\n",
    "parameterGrid = {'hidden_layer_sizes': unitParameters,  \n",
    "    'max_iter': epochParameters,\n",
    "    } "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA EXTRACTION\n",
    "\n",
    "# Importing the main dataframe\n",
    "dataFrameStockReturns = pd.read_csv(r'..\\PSE-SR.csv')\n",
    "dataFrameStockReturns = dataFrameStockReturns.dropna()\n",
    "dataFrameStockReturns['Date'] = pd.to_datetime(dataFrameStockReturns['Date'])\n",
    "\n",
    "# Separating the stock returns columns to append Persistent Homology input vectors\n",
    "stockReturnsColumns = ['PSEi_SR','PSE SER_SR','PSE IND_SR']\n",
    "dataFramePHVectors = dataFrameStockReturns.drop(stockReturnsColumns,axis=1)\n",
    "underSampler = RandomUnderSampler(random_state=42, sampling_strategy = 'majority')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running the code on each year and the dimensions\n",
    "\n",
    "gridResults = []\n",
    "\n",
    "for dimension in dimensionPH:\n",
    "\n",
    "    for year in range(startYear, endYearExclusive):\n",
    "\n",
    "        baseDFBestParams = filterDataOnYear(dataFramePHVectors, 'Date', year)\n",
    "\n",
    "        # Creating the persistent homology features in \n",
    "        for window in windowSizes:\n",
    "\n",
    "            #Converting the data to sliding windows\n",
    "            slidingWindow = SlidingWindow(size = window)\n",
    "            dataFrameByYear = filterDataSlidingWindow(dataFrameStockReturns, 'Date', year, window)\n",
    "            stockReturnsByYear = dataFrameByYear.loc[:, stockReturnsColumns].values.tolist()\n",
    "            stockReturnsForVR = np.array(slidingWindow.fit_transform(stockReturnsByYear))\n",
    "\n",
    "            # Initiating Vietoris Rips Persistence\n",
    "            VR = VietorisRipsPersistence(\n",
    "                    homology_dimensions=[dimension]\n",
    "                )\n",
    "            VRdiagrams = VR.fit_transform(stockReturnsForVR)\n",
    "            \n",
    "            # Creating partitions on generated Betti Curve coordinates\n",
    "            bettiSeqlist = getBettiSeq(VRdiagrams, dimension=dimension)\n",
    "            for partition in partitionSizes:\n",
    "                bettiSeqVectors = createBettiSeqPartition(bettiSeqlist, partition)\n",
    "                headerName = 'Hom'+str(dimension)+'_W'+ str(window) + '_P' + str(partition)  \n",
    "                baseDFBestParams[headerName] = bettiSeqVectors\n",
    "\n",
    "        # Running the model on all combinations of homology on the machine learning model\n",
    "        homColumns = baseDFBestParams.drop(['Date','Class'], axis=1)\n",
    "        \n",
    "        bestGridScore = 0\n",
    "        bestParams_svc = 0\n",
    "        bestParams_ph = None\n",
    "\n",
    "        for column in homColumns.columns:\n",
    "\n",
    "            # Setting and splitting the train and test data (with undersampling)\n",
    "            X = pd.DataFrame(homColumns[column])\n",
    "            y = baseDFBestParams.Class \n",
    "            X, y = underSampler.fit_resample(X, y)\n",
    "\n",
    "            # Splitting the training data of each year with training testing split (with under sampling)\n",
    "            XTrain, XTest, yTrain, yTest = train_test_split(X, y, test_size=0.5,stratify=y, random_state=5)\n",
    "            XTrain, yTrain = underSampler.fit_resample(XTrain, yTrain)\n",
    "            XTest, yTest = underSampler.fit_resample(XTest, yTest)\n",
    "            \n",
    "            # Apply scalers on the list of vectors\n",
    "            XTrain = list(XTrain[column])\n",
    "            XTest = list(XTest[column])\n",
    "            scaler = MinMaxScaler()\n",
    "            XTrain = scaler.fit_transform(XTrain)\n",
    "            XTest = scaler.transform(XTest)\n",
    "            \n",
    "            # Running the SVMs on the combinations on kernel functions, cost values and deg/gamma\n",
    "            classifier = MLPClassifier(activation='logistic', random_state=20, learning_rate_init=0.01)\n",
    "            gridSearch = GridSearchCV(\n",
    "                estimator=classifier,\n",
    "                param_grid=parameterGrid,\n",
    "                scoring='accuracy',\n",
    "                cv = StratifiedKFold(n_splits=12,shuffle=True, random_state=50),\n",
    "                n_jobs=10)\n",
    "            gridSearch.fit(XTrain, yTrain) \n",
    "            gridSearchScore = gridSearch.best_score_ \n",
    "\n",
    "            # Defining the variables for the Grid Search results\n",
    "            if gridSearchScore >= bestGridScore:\n",
    "                bestGridScore = gridSearchScore\n",
    "                bestParams_svc = gridSearch.best_params_\n",
    "                bestSVC = gridSearch.best_estimator_\n",
    "                bestParams_ph = str(column)\n",
    "\n",
    "        # Running the best model on the parameters to predict on the Test Data\n",
    "        bestSVC.fit(XTrain,yTrain)\n",
    "        bestTrainAcc = bestSVC.fit(XTrain,yTrain).score(XTrain,yTrain)\n",
    "        yPred = bestSVC.predict(XTest)\n",
    "        bestTestAcc = accuracy_score(yTest, yPred)\n",
    "\n",
    "        gridResults.append({\n",
    "            'Dimension': dimension,\n",
    "            'Year': year, \n",
    "            'Best Window and Partition': bestParams_ph, \n",
    "            'Best Parameters': bestParams_svc, \n",
    "            'Best Grid Search Score': bestGridScore, \n",
    "            'Train Acc on Best Model' : bestTrainAcc, \n",
    "            'Test Accuracy on Best Model': bestTestAcc\n",
    "        })\n",
    "\n",
    "gridResultsPerYear = pd.DataFrame(gridResults) \n",
    "\n",
    "print(scaler)\n",
    "gridResultsPerYear"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
